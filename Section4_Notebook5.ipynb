{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5: Optimization for Machine Learning\n",
    "\n",
    "## Learning Objectives\n",
    "- Understand convex vs non-convex optimization\n",
    "- Learn gradient descent and its variants\n",
    "- Explore loss functions used in ML\n",
    "- Understand and implement regularization\n",
    "- Apply optimization in practical examples\n",
    "\n",
    "We bridge calculus from Notebook 4 to real ML training algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Convex vs Non-Convex Optimization\n",
    "**Convex function:** A function \\(f(x)\\) where the line segment between any two points lies above the curve.\n",
    "\n",
    "**Why it matters:** In convex problems, any local minimum is also the global minimum — gradient descent is easier.\n",
    "\n",
    "**Non-convex:** Multiple local minima and saddle points — common in deep learning.\n",
    "\n",
    "### Visual intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.linspace(-3, 3, 200)\n",
    "convex_y = x**2\n",
    "nonconvex_y = np.sin(x) + 0.2*x\n",
    "\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(x, convex_y)\n",
    "plt.title('Convex Function: $y = x^2$')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(x, nonconvex_y)\n",
    "plt.title('Non-Convex Function: $y = sin(x) + 0.2x$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gradient Descent Algorithm\n",
    "\n",
    "**Update rule:**\n",
    "\\[ w := w - \\eta \\nabla_w L(w) \\]\n",
    "\n",
    "- \\(w\\): parameters\n",
    "- \\(\\eta\\): learning rate\n",
    "- \\(L(w)\\): loss function\n",
    "\n",
    "**Variants:**\n",
    "- Batch Gradient Descent\n",
    "- Stochastic Gradient Descent (SGD)\n",
    "- Mini-batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple Gradient Descent on y = x^2\n",
    "def gradient_descent(lr=0.1, steps=20):\n",
    "    x = 5.0  # start point\n",
    "    history = [x]\n",
    "    for _ in range(steps):\n",
    "        grad = 2*x  # derivative of x^2\n",
    "        x -= lr * grad\n",
    "        history.append(x)\n",
    "    return history\n",
    "\n",
    "history = gradient_descent(lr=0.2, steps=15)\n",
    "plt.plot(history, marker='o')\n",
    "plt.title('Gradient Descent on $y = x^2$')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('x value')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loss Functions in ML\n",
    "\n",
    "### Regression\n",
    "- Mean Squared Error (MSE): \\( \\frac{1}{n}\\sum (y - \\hat{y})^2 \\)\n",
    "- Mean Absolute Error (MAE)\n",
    "\n",
    "### Classification\n",
    "- Binary Cross-Entropy: \\( -\\frac{1}{n}\\sum y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y}) \\)\n",
    "- Categorical Cross-Entropy\n",
    "\n",
    "Loss choice impacts optimization surface."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizing MSE vs MAE surfaces for a simple dataset\n",
    "y_true = np.array([3.0, -0.5, 2.0])\n",
    "y_preds = np.linspace(-2, 5, 100)\n",
    "mse = [(y_true - yp)**2 for yp in y_preds]\n",
    "mae = [np.abs(y_true - yp) for yp in y_preds]\n",
    "\n",
    "plt.plot(y_preds, np.mean(mse, axis=1), label='MSE')\n",
    "plt.plot(y_preds, np.mean(mae, axis=1), label='MAE')\n",
    "plt.legend()\n",
    "plt.title('MSE vs MAE')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Regularization\n",
    "\n",
    "### Why?\n",
    "- Prevent overfitting\n",
    "- Penalize large weights\n",
    "\n",
    "### Types:\n",
    "- L1 (Lasso): Encourages sparsity, penalty = \\( \\lambda \\sum |w| \\)\n",
    "- L2 (Ridge): Encourages small weights, penalty = \\( \\lambda \\sum w^2 \\)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Adding L2 regularization to MSE\n",
    "def mse_with_l2(y_true, y_pred, weights, lam=0.1):\n",
    "    mse = np.mean((y_true - y_pred)**2)\n",
    "    l2_penalty = lam * np.sum(weights**2)\n",
    "    return mse + l2_penalty\n",
    "\n",
    "# Dummy example\n",
    "y_true = np.array([3, 2, 4])\n",
    "y_pred = np.array([2.5, 2.2, 3.8])\n",
    "weights = np.array([0.5, -0.3])\n",
    "mse_with_l2(y_true, y_pred, weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Exercise\n",
    "**Task:** Implement stochastic gradient descent (SGD) for a simple linear regression problem without using sklearn.\n",
    "\n",
    "**Dataset:** Generate your own synthetic data with noise.\n",
    "\n",
    "**Steps:**\n",
    "1. Initialize weights randomly.\n",
    "2. Loop over epochs:\n",
    "    - Shuffle data\n",
    "    - For each sample, compute gradient and update weights\n",
    "3. Track loss over time and plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your turn!\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# TODO: Implement SGD here\n",
    "lr = 0.1\n",
    "epochs = 50\n",
    "w = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "losses = []\n",
    "\n",
    "# Fill in loop\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
