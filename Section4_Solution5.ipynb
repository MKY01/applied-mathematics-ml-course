{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 5 — Solutions: Optimization for Machine Learning\n",
    "\n",
    "This notebook contains the worked-out solutions for the exercises in **Notebook 5**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise Solution — Implementing SGD for Linear Regression\n",
    "\n",
    "**Goal:** Learn how to implement Stochastic Gradient Descent (SGD) from scratch for a simple linear regression problem.\n",
    "\n",
    "### Key Steps:\n",
    "1. Initialize weights randomly.\n",
    "2. Loop over epochs.\n",
    "3. Shuffle the training data.\n",
    "4. For each sample, compute prediction, loss, gradient, and update weights.\n",
    "5. Track loss over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Recreate synthetic dataset\n",
    "np.random.seed(42)\n",
    "X = 2 * np.random.rand(100, 1)\n",
    "y = 4 + 3 * X + np.random.randn(100, 1)\n",
    "\n",
    "# SGD parameters\n",
    "lr = 0.1  # learning rate\n",
    "epochs = 50\n",
    "w = np.random.randn(1)  # weight\n",
    "b = np.random.randn(1)  # bias\n",
    "losses = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    # Shuffle data each epoch\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_shuffled = X[indices]\n",
    "    y_shuffled = y[indices]\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for i in range(X.shape[0]):\n",
    "        xi = X_shuffled[i]\n",
    "        yi = y_shuffled[i]\n",
    "\n",
    "        # Prediction\n",
    "        y_pred = w * xi + b\n",
    "\n",
    "        # Error\n",
    "        error = y_pred - yi\n",
    "\n",
    "        # Gradients\n",
    "        dw = 2 * xi * error\n",
    "        db = 2 * error\n",
    "\n",
    "        # Parameter update\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "\n",
    "        # Track loss (MSE for this sample)\n",
    "        epoch_loss += error**2\n",
    "\n",
    "    # Average loss for the epoch\n",
    "    losses.append(epoch_loss / X.shape[0])\n",
    "\n",
    "# Final parameters\n",
    "print(f\"Learned parameters: w = {w[0]:.4f}, b = {b[0]:.4f}\")\n",
    "\n",
    "# Plot loss curve\n",
    "plt.plot(losses)\n",
    "plt.title('SGD Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Explanation\n",
    "- **Learning rate (lr)** controls how big each step in gradient descent is.\n",
    "- **Weight (w)** and **bias (b)** are initialized randomly and refined over time.\n",
    "- Each epoch goes through the entire dataset, but **SGD** updates weights after **each sample**.\n",
    "- Shuffling prevents cycles and ensures better convergence.\n",
    "- Loss curve should **decrease smoothly** if the learning rate is reasonable.\n",
    "- The final parameters should be close to the true generating model (~3 for weight, ~4 for bias)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: Mini-Batch Gradient Descent\n",
    "\n",
    "SGD can be noisy since it updates after each single point.\n",
    "Mini-batch GD updates after a small batch (e.g., 10 samples).\n",
    "This balances computational efficiency and stable convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mini-batch implementation\n",
    "batch_size = 10\n",
    "w = np.random.randn(1)\n",
    "b = np.random.randn(1)\n",
    "losses_mb = []\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    indices = np.arange(X.shape[0])\n",
    "    np.random.shuffle(indices)\n",
    "    X_shuffled = X[indices]\n",
    "    y_shuffled = y[indices]\n",
    "\n",
    "    epoch_loss = 0\n",
    "    for start in range(0, X.shape[0], batch_size):\n",
    "        end = start + batch_size\n",
    "        X_batch = X_shuffled[start:end]\n",
    "        y_batch = y_shuffled[start:end]\n",
    "\n",
    "        y_pred = w * X_batch + b\n",
    "        error = y_pred - y_batch\n",
    "\n",
    "        dw = (2 / batch_size) * np.sum(X_batch * error)\n",
    "        db = (2 / batch_size) * np.sum(error)\n",
    "\n",
    "        w -= lr * dw\n",
    "        b -= lr * db\n",
    "\n",
    "        epoch_loss += np.mean(error**2)\n",
    "\n",
    "    losses_mb.append(epoch_loss / (X.shape[0] / batch_size))\n",
    "\n",
    "# Plot mini-batch loss curve\n",
    "plt.plot(losses_mb)\n",
    "plt.title('Mini-Batch GD Training Loss over Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MSE Loss')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
