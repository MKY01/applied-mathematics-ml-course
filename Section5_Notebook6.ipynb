{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6 — Exercises: Probability Models & ML Foundations\n",
    "\n",
    "## Learning Objectives\n",
    "By the end of this notebook, you should be able to:\n",
    "- Apply probability rules (joint, marginal, conditional) in ML contexts.\n",
    "- Derive and compute Maximum Likelihood Estimates (MLE).\n",
    "- Understand and implement Naive Bayes classification.\n",
    "- Compare MLE with Maximum A Posteriori (MAP) estimation.\n",
    "- Use Python to implement probability-based models."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability Refresher — Joint, Marginal, Conditional\n",
    "\n",
    "**Task 1.1:** Given the joint probability table below for two binary variables `A` and `B`, compute:\n",
    "- Marginal probabilities `P(A=1)` and `P(B=1)`\n",
    "- Conditional probability `P(A=1 | B=1)`\n",
    "\n",
    "| A | B | P(A,B) |\n",
    "|---|---|--------|\n",
    "| 0 | 0 | 0.1    |\n",
    "| 0 | 1 | 0.3    |\n",
    "| 1 | 0 | 0.2    |\n",
    "| 1 | 1 | 0.4    |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 1.1 — Your code here\n",
    "import numpy as np\n",
    "\n",
    "joint_probs = np.array([\n",
    "    [0.1, 0.3],  # A=0,B=0 and A=0,B=1\n",
    "    [0.2, 0.4]   # A=1,B=0 and A=1,B=1\n",
    "])\n",
    "\n",
    "# TODO: Compute P(A=1), P(B=1), P(A=1|B=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayes' Theorem in ML Context\n",
    "\n",
    "A spam filter uses the probability of words appearing in spam vs. non-spam emails.\n",
    "\n",
    "**Task 2.1:** Using Bayes' theorem, compute:\n",
    "\\[ P(\\text{spam} | \\text{word='offer'}) \\]\n",
    "Given:\n",
    "- P(spam) = 0.4\n",
    "- P(word='offer' | spam) = 0.6\n",
    "- P(word='offer' | not spam) = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 2.1 — Your code here\n",
    "P_spam = 0.4\n",
    "P_offer_given_spam = 0.6\n",
    "P_offer_given_not_spam = 0.05\n",
    "\n",
    "# TODO: Use Bayes' theorem to compute P(spam | offer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Maximum Likelihood Estimation (MLE)\n",
    "\n",
    "**Task 3.1:** You toss a coin 10 times and get 7 heads. Estimate the probability of heads using MLE for the Bernoulli distribution.\n",
    "\n",
    "**Task 3.2:** Generate 100 samples from a normal distribution with mean 5 and std 2. Compute the MLE estimates for the mean and variance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.1 — Bernoulli MLE\n",
    "heads = 7\n",
    "n = 10\n",
    "\n",
    "# TODO: MLE for Bernoulli parameter p\n",
    "\n",
    "# Task 3.2 — Gaussian MLE\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(5, 2, 100)\n",
    "\n",
    "# TODO: MLE for mean and variance\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Maximum A Posteriori (MAP) Estimation\n",
    "\n",
    "**Task 4.1:** Suppose you have the same coin-toss experiment (7 heads out of 10), but you also have a prior belief that the coin is fair (p=0.5) with a Beta(α=2, β=2) prior.\n",
    "Compute the MAP estimate for p."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 4.1 — MAP estimation with Beta prior\n",
    "alpha_prior = 2\n",
    "beta_prior = 2\n",
    "\n",
    "# TODO: Compute MAP for p\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes Classifier from Scratch\n",
    "\n",
    "**Task 5.1:** Implement a simple Naive Bayes classifier for binary classification using discrete features.\n",
    "- Create a small dataset with binary features and binary labels.\n",
    "- Compute class priors and likelihoods.\n",
    "- Predict labels for test samples.\n",
    "\n",
    "**Task 5.2:** Compare your results with `sklearn.naive_bayes.BernoulliNB`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 5.1 — Implement Naive Bayes from scratch\n",
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "# Example dataset\n",
    "X_train = np.array([\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "y_train = np.array([1, 1, 0, 0])\n",
    "\n",
    "# TODO: Implement Naive Bayes manually\n",
    "\n",
    "# Task 5.2 — Compare to sklearn\n",
    "clf = BernoulliNB()\n",
    "clf.fit(X_train, y_train)\n",
    "print(\"sklearn predictions:\", clf.predict(X_train))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
