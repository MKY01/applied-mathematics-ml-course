{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook 6 — Solutions: Probability Models & ML Foundations\n",
    "\n",
    "Worked solutions and explanations for the tasks in Notebook 6 (Exercises)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Probability Refresher — Joint, Marginal, Conditional (Task 1.1)\n",
    "\n",
    "Given joint probability table for binary A and B:\n",
    "\n",
    "| A | B | P(A,B) |\n",
    "|---|---|--------|\n",
    "| 0 | 0 | 0.1    |\n",
    "| 0 | 1 | 0.3    |\n",
    "| 1 | 0 | 0.2    |\n",
    "| 1 | 1 | 0.4    |\n",
    "\n",
    "Compute:\n",
    "- Marginal P(A=1), P(B=1)\n",
    "- Conditional P(A=1 | B=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "joint_probs = np.array([\n",
    "    [0.1, 0.3],  # A=0, B=0 ; A=0, B=1\n",
    "    [0.2, 0.4]   # A=1, B=0 ; A=1, B=1\n",
    "])\n",
    "\n",
    "# Marginal P(A=1) = sum over B of P(A=1, B)\n",
    "P_A1 = joint_probs[1, :].sum()\n",
    "\n",
    "# Marginal P(B=1) = sum over A of P(A, B=1)\n",
    "P_B1 = joint_probs[:, 1].sum()\n",
    "\n",
    "# Conditional P(A=1 | B=1) = P(A=1,B=1) / P(B=1)\n",
    "P_A1_given_B1 = joint_probs[1, 1] / P_B1\n",
    "\n",
    "print(f\"P(A=1) = {P_A1:.3f}\")\n",
    "print(f\"P(B=1) = {P_B1:.3f}\")\n",
    "print(f\"P(A=1 | B=1) = {P_A1_given_B1:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- P(A=1) = 0.2 + 0.4 = 0.6\n",
    "- P(B=1) = 0.3 + 0.4 = 0.7\n",
    "- P(A=1 | B=1) = 0.4 / 0.7 ≈ 0.571"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Bayes' Theorem in an ML Context (Task 2.1)\n",
    "\n",
    "Compute P(spam | word='offer') given:\n",
    "- P(spam) = 0.4\n",
    "- P(offer | spam) = 0.6\n",
    "- P(offer | not spam) = 0.05\n",
    "\n",
    "Use Bayes' rule:\n",
    "\\[ P(spam|offer) = \\frac{P(offer|spam)P(spam)}{P(offer)} \\]\n",
    "where \\(P(offer) = P(offer|spam)P(spam) + P(offer|not\\;spam)P(not\\;spam)\\)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "P_spam = 0.4\n",
    "P_offer_given_spam = 0.6\n",
    "P_offer_given_not_spam = 0.05\n",
    "\n",
    "P_not_spam = 1 - P_spam\n",
    "P_offer = P_offer_given_spam * P_spam + P_offer_given_not_spam * P_not_spam\n",
    "P_spam_given_offer = (P_offer_given_spam * P_spam) / P_offer\n",
    "\n",
    "print(f\"P(offer) = {P_offer:.4f}\")\n",
    "print(f\"P(spam | offer) = {P_spam_given_offer:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "Numerator: 0.6 * 0.4 = 0.24\n",
    "Denominator: 0.6*0.4 + 0.05*0.6 = 0.24 + 0.03 = 0.27\n",
    "So P(spam|offer) = 0.24 / 0.27 ≈ 0.8889"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Maximum Likelihood Estimation (Task 3.1 & 3.2)\n",
    "\n",
    "**Task 3.1 (Bernoulli MLE):** 7 heads out of 10 tosses. MLE for p is the sample proportion \\(\\hat{p} = k/n\\).\n",
    "\n",
    "**Task 3.2 (Gaussian MLE):** For data from N(μ, σ²), MLE estimates are:\n",
    "- \\(\\hat{\\mu} = \\frac{1}{n} \\sum x_i\\)\n",
    "- \\(\\hat{\\sigma}^2 = \\frac{1}{n} \\sum (x_i - \\hat{\\mu})^2\\)  (use 1/n for MLE, not 1/(n-1))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task 3.1: Bernoulli MLE\n",
    "heads = 7\n",
    "n = 10\n",
    "p_hat = heads / n\n",
    "print(f\"MLE for Bernoulli p_hat = {p_hat:.3f}\")\n",
    "\n",
    "# Task 3.2: Gaussian MLE\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(5, 2, 100)\n",
    "mu_mle = np.mean(data)\n",
    "var_mle = np.mean((data - mu_mle)**2)  # MLE uses 1/n\n",
    "print(f\"Gaussian MLE: mu = {mu_mle:.4f}, var = {var_mle:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Bernoulli: p_hat = 7/10 = 0.7\n",
    "- Gaussian: use sample mean and 1/n variance for MLE (not unbiased sample variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Maximum A Posteriori (Task 4.1)\n",
    "\n",
    "We have a Beta(α, β) prior. For Bernoulli likelihood with k heads and n trials, the posterior is Beta(α + k, β + n - k).\n",
    "\n",
    "MAP estimate for Beta(α, β) posterior (when α>1 and β>1) is:\n",
    "\\[ p_{MAP} = \\frac{\\alpha_{post} - 1}{\\alpha_{post} + \\beta_{post} - 2} \\]\n",
    "\n",
    "Given α_prior=2, β_prior=2, k=7, n=10 → α_post=9, β_post=5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha_prior = 2\n",
    "beta_prior = 2\n",
    "heads = 7\n",
    "n = 10\n",
    "\n",
    "alpha_post = alpha_prior + heads\n",
    "beta_post = beta_prior + (n - heads)\n",
    "p_map = (alpha_post - 1) / (alpha_post + beta_post - 2)\n",
    "\n",
    "print(f\"Posterior Beta(α={alpha_post}, β={beta_post})\")\n",
    "print(f\"MAP estimate = {p_map:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation:**\n",
    "- Posterior Beta(9,5)\n",
    "- MAP = (9-1)/(9+5-2) = 8/12 = 0.6667\n",
    "- Note: MLE gave 0.7; the prior (favoring fairness) pulls the MAP slightly toward 0.5."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Naive Bayes Classifier from Scratch (Task 5.1 & 5.2)\n",
    "\n",
    "We'll implement a Bernoulli Naive Bayes for binary features (with Laplace smoothing). Steps:\n",
    "1. Compute class priors P(y=c).\n",
    "2. For each feature j and class c compute P(x_j=1 | y=c) with Laplace smoothing.\n",
    "3. For prediction compute log P(y=c) + sum_j log P(x_j | y=c) and take argmax.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "\n",
    "X_train = np.array([\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0]\n",
    "])\n",
    "y_train = np.array([1, 1, 0, 0])\n",
    "\n",
    "def bernoulli_nb_train(X, y, alpha=1.0):\n",
    "    # alpha is Laplace smoothing parameter\n",
    "    classes = np.unique(y)\n",
    "    n_classes = len(classes)\n",
    "    n_features = X.shape[1]\n",
    "    priors = {}\n",
    "    likelihoods = {}  # dict: class -> array of P(x_j=1 | class)\n",
    "    for c in classes:\n",
    "        X_c = X[y == c]\n",
    "        priors[c] = X_c.shape[0] / X.shape[0]\n",
    "        # Laplace smoothing: (count + alpha) / (N_c + 2*alpha) for Bernoulli\n",
    "        counts = X_c.sum(axis=0)\n",
    "        likelihoods[c] = (counts + alpha) / (X_c.shape[0] + 2 * alpha)\n",
    "    return classes, priors, likelihoods\n",
    "\n",
    "def bernoulli_nb_predict(X, classes, priors, likelihoods):\n",
    "    preds = []\n",
    "    for x in X:\n",
    "        class_log_probs = {}\n",
    "        for c in classes:\n",
    "            logp = np.log(priors[c])\n",
    "            p_x_given_c = likelihoods[c]\n",
    "            # For each feature j: if x_j==1 use p_j, else use (1-p_j)\n",
    "            logp += np.sum(np.log(p_x_given_c) * (x == 1))\n",
    "            logp += np.sum(np.log(1 - p_x_given_c) * (x == 0))\n",
    "            class_log_probs[c] = logp\n",
    "        # choose class with max log-prob\n",
    "        pred = max(class_log_probs.items(), key=lambda kv: kv[1])[0]\n",
    "        preds.append(pred)\n",
    "    return np.array(preds)\n",
    "\n",
    "# Train our implementation\n",
    "classes, priors, likelihoods = bernoulli_nb_train(X_train, y_train, alpha=1.0)\n",
    "print(\"Classes:\", classes)\n",
    "print(\"Priors:\", priors)\n",
    "print(\"Likelihoods (P(x_j=1|class)):\")\n",
    "for c in classes:\n",
    "    print(c, likelihoods[c])\n",
    "\n",
    "# Predict on training data\n",
    "preds_manual = bernoulli_nb_predict(X_train, classes, priors, likelihoods)\n",
    "print(\"Manual NB predictions:\", preds_manual)\n",
    "\n",
    "# Compare to sklearn BernoulliNB\n",
    "clf = BernoulliNB(alpha=1.0)\n",
    "clf.fit(X_train, y_train)\n",
    "sk_preds = clf.predict(X_train)\n",
    "print(\"sklearn predictions:\", sk_preds)\n",
    "\n",
    "print(\"Are predictions identical?\", np.array_equal(preds_manual, sk_preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation & Notes:**\n",
    "- Priors: classes appear equally often (2/4 each) → priors 0.5 each.\n",
    "- Likelihoods (with Laplace smoothing) estimate P(feature=1 | class).\n",
    "- Prediction uses log-probabilities to avoid underflow.\n",
    "- Sklearn's `BernoulliNB` by default uses smoothing (alpha) and may compute class log priors similarly; predictions should match in this small example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary & Takeaways\n",
    "- You practiced computing marginals & conditional probabilities from joint distributions.\n",
    "- Applied Bayes' theorem to compute posterior probabilities in an ML example.\n",
    "- Computed MLEs for Bernoulli and Gaussian models and saw the difference with MAP when using a prior.\n",
    "- Implemented a Bernoulli Naive Bayes classifier from scratch and validated against scikit-learn.\n",
    "\n",
    "You're now ready to move from probability-based models to the broader **Core Machine Learning Algorithms** section — we'll cover linear/logistic regression, decision trees, ensembles, clustering, evaluation metrics, and practical model-building next.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
